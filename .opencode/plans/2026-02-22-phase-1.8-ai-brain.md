# Phase 1.8: The AI Brain (Reasoning Engine) — Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Implement the LLM agent that orchestrates MCP tools, performs cross-app reasoning, generates "Tough Love Coach" responses, and manages rate limits/usage tracking.

**Architecture:** The Orchestrator scaffold (Phase 1.3) gets upgraded with a real LLM conversation loop. The `openai` SDK connects to OpenRouter (Kimi K2.5) with function-calling. A ReAct-style tool execution loop (max 5 turns) routes tool calls through the existing `MCPClient`. Rate limiting uses a new Redis-backed per-user/tier service alongside the existing `slowapi` IP limiter.

**Tech Stack:** Python 3.12, FastAPI, `openai` SDK (AsyncOpenAI), Redis (`redis-py`), SQLAlchemy 2.0 (async), pytest + pytest-asyncio, Flutter/Dart (harness UI).

**Branch:** `feat/phase-1.8-ai-brain`

**Deviations from Original Plan:**
1. **LLM Client:** Uses `openai` SDK (`AsyncOpenAI`) instead of raw `httpx` + `tenacity`. The SDK provides built-in retries, streaming, structured `tool_calls` parsing, and type safety — reducing custom code by ~60%.
2. **Phase 1.8.8 (Kimi Integration Doc):** Already exists at `docs/plans/backend/integrations/ai-brain-integration.md` (368 lines). Will update rather than create.
3. **Rate Limiter:** Keeps existing `slowapi` for IP-level abuse protection; adds new Redis-based per-user/tier limiter for LLM cost control (different concerns).
4. **Voice Input (1.8.5):** Scaffold with mock transcription. Real Whisper integration deferred until infrastructure is ready.

**Dependencies from prior phases:**
- Phase 1.3: `MCPClient`, `MCPServerRegistry`, `MemoryStore`/`InMemoryStore`, `Orchestrator` scaffold
- Phase 1.6: `StravaServer` MCP integration
- Phase 1.7: CalAI nutrition data via Health Store
- Phase 1.9: `chat.py` WebSocket endpoint, `Conversation`/`Message` models, `WsClient` (Flutter)

---

## Task 1: Branch Setup & Dependency Installation

**Files:**
- Modify: `cloud-brain/pyproject.toml`
- Modify: `cloud-brain/.env.example`
- Modify: `cloud-brain/app/config.py`

**Step 1: Create feature branch**

```bash
git checkout -b feat/phase-1.8-ai-brain
```

**Step 2: Add new Python dependencies**

Add to `cloud-brain/pyproject.toml` dependencies:

```toml
"openai>=1.60.0",
"redis[hiredis]>=5.0.0",
"python-multipart>=0.0.9",
```

- `openai`: AsyncOpenAI client for OpenRouter + Whisper STT
- `redis[hiredis]`: Async Redis for per-user rate limiting
- `python-multipart`: Required by FastAPI for `UploadFile` (voice input)

**Step 3: Run dependency install**

```bash
cd cloud-brain
uv sync
```
Expected: All dependencies resolve and install.

**Step 4: Add OpenRouter config to `.env.example`**

Append to `cloud-brain/.env.example`:

```env
# --- OpenRouter / AI Brain (Phase 1.8) ---
OPENROUTER_API_KEY=your-openrouter-api-key
OPENROUTER_REFERER=https://lifelogger.app
OPENROUTER_TITLE=Life Logger
OPENROUTER_MODEL=moonshot/kimi-k2.5
```

**Step 5: Add Settings fields to `cloud-brain/app/config.py`**

Add these fields to the `Settings` class after `openai_api_key`:

```python
openrouter_api_key: str = ""
openrouter_referer: str = "https://lifelogger.app"
openrouter_title: str = "Life Logger"
openrouter_model: str = "moonshot/kimi-k2.5"
```

**Step 6: Commit**

```bash
git add -A
git commit -m "chore(phase-1.8): add openai, redis, multipart deps and OpenRouter config"
```

---

## Task 2: LLM Client Setup (Sub-phase 1.8.1)

**Files:**
- Create: `cloud-brain/app/agent/llm_client.py`
- Create: `cloud-brain/tests/test_llm_client.py`

**Step 1: Write the failing test**

Create `cloud-brain/tests/test_llm_client.py`:

```python
"""
Life Logger Cloud Brain — LLM Client Tests.

Unit tests for the LLMClient wrapper around AsyncOpenAI.
All tests mock the OpenAI SDK to avoid real API calls.
"""

from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from app.agent.llm_client import LLMClient


@pytest.fixture
def llm_client():
    """Create an LLMClient with mocked settings."""
    with patch("app.agent.llm_client.settings") as mock_settings:
        mock_settings.openrouter_api_key = "test-key"
        mock_settings.openrouter_referer = "https://test.app"
        mock_settings.openrouter_title = "Test App"
        mock_settings.openrouter_model = "moonshot/kimi-k2.5"
        client = LLMClient()
        yield client


@pytest.mark.asyncio
async def test_chat_returns_message(llm_client):
    """chat() should return the assistant message content."""
    mock_response = MagicMock()
    mock_choice = MagicMock()
    mock_choice.message.content = "Hello, I'm your coach!"
    mock_choice.message.tool_calls = None
    mock_response.choices = [mock_choice]
    mock_response.usage.prompt_tokens = 50
    mock_response.usage.completion_tokens = 20

    llm_client._client.chat.completions.create = AsyncMock(
        return_value=mock_response
    )

    messages = [{"role": "user", "content": "Hi"}]
    response = await llm_client.chat(messages)

    assert response.choices[0].message.content == "Hello, I'm your coach!"
    llm_client._client.chat.completions.create.assert_called_once()


@pytest.mark.asyncio
async def test_chat_passes_tools(llm_client):
    """chat() should forward tool definitions to the API."""
    mock_response = MagicMock()
    mock_choice = MagicMock()
    mock_choice.message.content = "Let me check your steps."
    mock_choice.message.tool_calls = None
    mock_response.choices = [mock_choice]
    mock_response.usage.prompt_tokens = 100
    mock_response.usage.completion_tokens = 30

    llm_client._client.chat.completions.create = AsyncMock(
        return_value=mock_response
    )

    tools = [
        {
            "type": "function",
            "function": {
                "name": "read_steps",
                "description": "Read step count",
                "parameters": {"type": "object", "properties": {}},
            },
        }
    ]
    messages = [{"role": "user", "content": "How many steps?"}]
    await llm_client.chat(messages, tools=tools)

    call_kwargs = llm_client._client.chat.completions.create.call_args[1]
    assert call_kwargs["tools"] == tools


@pytest.mark.asyncio
async def test_chat_without_tools(llm_client):
    """chat() without tools should not pass tools parameter."""
    mock_response = MagicMock()
    mock_choice = MagicMock()
    mock_choice.message.content = "Sure!"
    mock_choice.message.tool_calls = None
    mock_response.choices = [mock_choice]
    mock_response.usage.prompt_tokens = 20
    mock_response.usage.completion_tokens = 5

    llm_client._client.chat.completions.create = AsyncMock(
        return_value=mock_response
    )

    messages = [{"role": "user", "content": "Hello"}]
    await llm_client.chat(messages)

    call_kwargs = llm_client._client.chat.completions.create.call_args[1]
    assert "tools" not in call_kwargs or call_kwargs.get("tools") is None


def test_default_model(llm_client):
    """LLMClient should use the configured default model."""
    assert llm_client.model == "moonshot/kimi-k2.5"


def test_custom_model():
    """LLMClient should accept a custom model override."""
    with patch("app.agent.llm_client.settings") as mock_settings:
        mock_settings.openrouter_api_key = "test-key"
        mock_settings.openrouter_referer = "https://test.app"
        mock_settings.openrouter_title = "Test App"
        mock_settings.openrouter_model = "moonshot/kimi-k2.5"
        client = LLMClient(model="google/gemini-flash-1.5")
        assert client.model == "google/gemini-flash-1.5"
```

**Step 2: Run test to verify it fails**

```bash
cd cloud-brain
python -m pytest tests/test_llm_client.py -v
```
Expected: FAIL — `ModuleNotFoundError: No module named 'app.agent.llm_client'`

**Step 3: Write the LLM client implementation**

Create `cloud-brain/app/agent/llm_client.py`:

```python
"""
Life Logger Cloud Brain — LLM Client.

Async wrapper around the OpenAI SDK configured for OpenRouter.
Provides a model-agnostic interface for chat completions with
optional function-calling (tool use). Uses AsyncOpenAI for
built-in retries, streaming, and structured tool_call parsing.

The client is designed to be instantiated once during application
lifespan and shared across requests.
"""

import logging
from typing import Any

from openai import AsyncOpenAI

from app.config import settings

logger = logging.getLogger(__name__)


class LLMClient:
    """Async LLM client wrapping OpenRouter via the OpenAI SDK.

    Uses AsyncOpenAI pointed at the OpenRouter base URL. Supports
    chat completions with optional tool definitions for function-calling.

    Attributes:
        model: The default model identifier (e.g. 'moonshot/kimi-k2.5').
        _client: The underlying AsyncOpenAI client instance.
    """

    def __init__(self, model: str | None = None) -> None:
        """Create a new LLM client.

        Args:
            model: Override the default model from settings.
                Defaults to ``settings.openrouter_model``.
        """
        self.model: str = model or settings.openrouter_model
        self._client = AsyncOpenAI(
            api_key=settings.openrouter_api_key,
            base_url="https://openrouter.ai/api/v1",
            default_headers={
                "HTTP-Referer": settings.openrouter_referer,
                "X-Title": settings.openrouter_title,
            },
            max_retries=3,
            timeout=60.0,
        )

    async def chat(
        self,
        messages: list[dict[str, Any]],
        tools: list[dict[str, Any]] | None = None,
        temperature: float = 0.7,
    ) -> Any:
        """Send a chat completion request to the LLM.

        Args:
            messages: The conversation history in OpenAI message format.
            tools: Optional list of tool definitions for function-calling.
            temperature: Sampling temperature (0.0-2.0). Defaults to 0.7.

        Returns:
            The full ChatCompletion response object from the OpenAI SDK.

        Raises:
            openai.APIError: On API communication failures (after retries).
        """
        kwargs: dict[str, Any] = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
        }

        if tools:
            kwargs["tools"] = tools

        logger.debug(
            "LLM request: model=%s, messages=%d, tools=%s",
            self.model,
            len(messages),
            len(tools) if tools else 0,
        )

        response = await self._client.chat.completions.create(**kwargs)

        logger.info(
            "LLM response: model=%s, tokens_in=%d, tokens_out=%d",
            self.model,
            response.usage.prompt_tokens if response.usage else 0,
            response.usage.completion_tokens if response.usage else 0,
        )

        return response

    async def stream_chat(
        self,
        messages: list[dict[str, Any]],
        tools: list[dict[str, Any]] | None = None,
        temperature: float = 0.7,
    ) -> Any:
        """Stream a chat completion response from the LLM.

        Returns an async iterator of chat completion chunks for
        lower-latency token-by-token delivery.

        Args:
            messages: The conversation history in OpenAI message format.
            tools: Optional list of tool definitions for function-calling.
            temperature: Sampling temperature. Defaults to 0.7.

        Returns:
            An async stream of ChatCompletionChunk objects.
        """
        kwargs: dict[str, Any] = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "stream": True,
        }

        if tools:
            kwargs["tools"] = tools

        return await self._client.chat.completions.create(**kwargs)
```

**Step 4: Run test to verify it passes**

```bash
cd cloud-brain
python -m pytest tests/test_llm_client.py -v
```
Expected: 5 tests PASS

**Step 5: Commit**

```bash
git add -A
git commit -m "feat(1.8.1): add LLM client with AsyncOpenAI for OpenRouter"
```

---

## Task 3: Agent System Prompt (Sub-phase 1.8.2)

**Files:**
- Create: `cloud-brain/app/agent/prompts/__init__.py`
- Create: `cloud-brain/app/agent/prompts/system.py`
- Create: `cloud-brain/tests/test_system_prompt.py`

**Step 1: Write the failing test**

Create `cloud-brain/tests/test_system_prompt.py`:

```python
"""
Life Logger Cloud Brain — System Prompt Tests.

Verifies the system prompt contains required persona elements,
capability descriptions, and safety constraints.
"""

import pytest

from app.agent.prompts.system import SYSTEM_PROMPT, build_system_prompt


def test_system_prompt_contains_persona():
    """System prompt must define the Tough Love Coach persona."""
    assert "Tough Love" in SYSTEM_PROMPT
    assert "direct" in SYSTEM_PROMPT.lower()


def test_system_prompt_contains_capabilities():
    """System prompt must mention key data sources."""
    assert "Apple Health" in SYSTEM_PROMPT or "apple_health" in SYSTEM_PROMPT
    assert "Strava" in SYSTEM_PROMPT or "strava" in SYSTEM_PROMPT
    assert "Health Connect" in SYSTEM_PROMPT or "health_connect" in SYSTEM_PROMPT


def test_system_prompt_contains_safety():
    """System prompt must include medical disclaimer."""
    lower = SYSTEM_PROMPT.lower()
    assert "medical" in lower or "doctor" in lower


def test_system_prompt_has_tool_rules():
    """System prompt must instruct the AI to use tools for data."""
    lower = SYSTEM_PROMPT.lower()
    assert "tool" in lower


def test_build_system_prompt_default():
    """build_system_prompt() without suffix returns base prompt."""
    result = build_system_prompt()
    assert result == SYSTEM_PROMPT


def test_build_system_prompt_with_suffix():
    """build_system_prompt() with suffix appends user context."""
    suffix = "\nUser Goal: Lose 5kg. Tone: Gentle."
    result = build_system_prompt(user_context_suffix=suffix)
    assert SYSTEM_PROMPT in result
    assert "Lose 5kg" in result
    assert "Gentle" in result
```

**Step 2: Run test to verify it fails**

```bash
cd cloud-brain
python -m pytest tests/test_system_prompt.py -v
```
Expected: FAIL — `ModuleNotFoundError`

**Step 3: Create the system prompt module**

Create `cloud-brain/app/agent/prompts/__init__.py`:
```python
"""Life Logger Cloud Brain — Agent Prompt Templates."""
```

Create `cloud-brain/app/agent/prompts/system.py`:

```python
"""
Life Logger Cloud Brain — System Prompt Definition.

Defines the core persona and behavioral rules for the AI agent.
This prompt is injected as the first message in every conversation.
The 'build_system_prompt()' function allows dynamic customization
based on user preferences (persona slider, goals).
"""

SYSTEM_PROMPT = """You are Life Logger, an AI health assistant with a "Tough Love Coach" persona.

## Who You Are
- You are direct, opinionated, and data-driven.
- You care deeply about the user's success but won't sugarcoat failure.
- You are NOT a medical doctor. Always disclaim medical advice.
- You speak with confidence but back every claim with data from your tools.

## Your Capabilities
You have access to the following tools via MCP (Model Context Protocol):

1. **Apple Health / Google Health Connect:** Read steps, workouts, sleep, weight, and nutrition data.
   - Tools: `read_metrics` (with data_type: steps, workouts, sleep, weight, nutrition)
2. **Strava:** Fetch running/cycling activities, create manual activities.
   - Tools: `get_activities`, `create_activity`
3. **CalAI (via Health Store):** See what users ate (nutrition entries written by CalAI to the Health Store).
4. **Memory:** Remember user goals, preferences, and past conversations.
   - Tools: `save_memory`, `query_memory`
5. **Deep Links:** Open external apps (CalAI camera, Strava recording).

## Rules of Engagement
1. **Check Data First:** If a user asks "How am I doing?", DO NOT guess. Use your tools to fetch their actual stats before responding.
2. **Be Specific:** Don't say "You moved a lot." Say "You hit 12,400 steps, which is 24% above your 10,000 daily goal."
3. **Cross-Reference:** If weight is up, check sleep AND nutrition AND activity. Find the *why*, don't just report the *what*.
4. **Action Over Talk:** Always end with a concrete challenge, next step, or question. Never leave the user without direction.
5. **Never Fabricate Data:** If a tool call fails or returns no data, say so honestly. Do NOT invent numbers.
6. **Ask Before Writing:** Before writing data to Health stores or creating Strava activities, confirm with the user first.
7. **Be Concise:** Health coaching is not an essay. Short, punchy responses with data.

## Tool Usage Guidelines
- Use `read_metrics` with appropriate `data_type` for daily health stats.
- Use `get_activities` for specific workout details from Strava.
- Use `save_memory` to remember critical user preferences and goals.
- When multiple data sources are needed, call tools in sequence — don't guess correlations.

## Tone Examples
- GOOD: "Listen, you missed your step goal 3 days in a row. It's raining, I get it — but you have a treadmill. No excuses. 30 minutes, go."
- BAD: "It looks like you didn't walk much. Maybe try to walk more?"
- GOOD: "Your CalAI data shows 2,400 cal yesterday but your maintenance is ~1,900 with only a 2km walk. That's a 500 cal surplus. Want me to set a target?"
- BAD: "You might be eating too much. Consider eating less."
"""


def build_system_prompt(user_context_suffix: str | None = None) -> str:
    """Build the complete system prompt with optional user context.

    Combines the base persona prompt with user-specific context
    such as coaching tone preferences and active goals.

    Args:
        user_context_suffix: Optional text to append to the base prompt.
            Typically generated by UserProfileManager based on stored
            user preferences (persona slider, goals, connected apps).

    Returns:
        The complete system prompt string ready for injection.
    """
    if user_context_suffix:
        return SYSTEM_PROMPT + user_context_suffix
    return SYSTEM_PROMPT
```

**Step 4: Run test to verify it passes**

```bash
cd cloud-brain
python -m pytest tests/test_system_prompt.py -v
```
Expected: 6 tests PASS

**Step 5: Commit**

```bash
git add -A
git commit -m "feat(1.8.2): define Tough Love Coach system prompt with dynamic builder"
```

---

## Task 4: Tool Selection Logic — Orchestrator Upgrade (Sub-phase 1.8.3)

**Files:**
- Modify: `cloud-brain/app/agent/orchestrator.py`
- Create: `cloud-brain/tests/test_orchestrator.py`

This is the most critical task. The Orchestrator scaffold gets replaced with a real ReAct-style function-calling loop.

**Step 1: Write the failing test**

Create `cloud-brain/tests/test_orchestrator.py`:

```python
"""
Life Logger Cloud Brain — Orchestrator Tests.

Tests the ReAct-style tool execution loop in the Orchestrator.
All LLM and MCP calls are mocked.
"""

import json
from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from app.agent.orchestrator import Orchestrator


@pytest.fixture
def mock_mcp_client():
    """Create a mocked MCPClient."""
    client = MagicMock()
    client.get_all_tools.return_value = []
    client.execute_tool = AsyncMock()
    return client


@pytest.fixture
def mock_memory_store():
    """Create a mocked MemoryStore."""
    store = MagicMock()
    store.query = AsyncMock(return_value=[])
    return store


@pytest.fixture
def mock_llm_client():
    """Create a mocked LLMClient."""
    client = MagicMock()
    client.chat = AsyncMock()
    return client


@pytest.fixture
def orchestrator(mock_mcp_client, mock_memory_store, mock_llm_client):
    """Create an Orchestrator with all mocked dependencies."""
    return Orchestrator(
        mcp_client=mock_mcp_client,
        memory_store=mock_memory_store,
        llm_client=mock_llm_client,
    )


@pytest.mark.asyncio
async def test_simple_text_response(orchestrator, mock_llm_client):
    """Orchestrator returns LLM text when no tool calls are made."""
    mock_message = MagicMock()
    mock_message.content = "You walked 10,000 steps today. Great job!"
    mock_message.tool_calls = None

    mock_response = MagicMock()
    mock_response.choices = [MagicMock(message=mock_message)]
    mock_response.usage.prompt_tokens = 100
    mock_response.usage.completion_tokens = 30

    mock_llm_client.chat.return_value = mock_response

    result = await orchestrator.process_message("user-1", "How are my steps?")
    assert result == "You walked 10,000 steps today. Great job!"


@pytest.mark.asyncio
async def test_single_tool_call(orchestrator, mock_mcp_client, mock_llm_client):
    """Orchestrator executes a single tool call and returns final response."""
    # First LLM response: tool call
    tool_call = MagicMock()
    tool_call.id = "call_123"
    tool_call.function.name = "read_metrics"
    tool_call.function.arguments = json.dumps({"data_type": "steps"})

    msg_with_tool = MagicMock()
    msg_with_tool.content = None
    msg_with_tool.tool_calls = [tool_call]

    response_1 = MagicMock()
    response_1.choices = [MagicMock(message=msg_with_tool)]
    response_1.usage.prompt_tokens = 100
    response_1.usage.completion_tokens = 20

    # Second LLM response: final text
    msg_final = MagicMock()
    msg_final.content = "You hit 12,400 steps today!"
    msg_final.tool_calls = None

    response_2 = MagicMock()
    response_2.choices = [MagicMock(message=msg_final)]
    response_2.usage.prompt_tokens = 200
    response_2.usage.completion_tokens = 25

    mock_llm_client.chat.side_effect = [response_1, response_2]

    # MCP tool result
    tool_result = MagicMock()
    tool_result.success = True
    tool_result.data = {"steps": 12400}
    tool_result.error = None
    mock_mcp_client.execute_tool.return_value = tool_result

    result = await orchestrator.process_message("user-1", "How many steps?")
    assert result == "You hit 12,400 steps today!"
    mock_mcp_client.execute_tool.assert_called_once()


@pytest.mark.asyncio
async def test_max_turns_safety(orchestrator, mock_llm_client):
    """Orchestrator stops after max turns to prevent infinite loops."""
    # Create a response that always requests tool calls
    tool_call = MagicMock()
    tool_call.id = "call_loop"
    tool_call.function.name = "read_metrics"
    tool_call.function.arguments = json.dumps({})

    msg_loop = MagicMock()
    msg_loop.content = None
    msg_loop.tool_calls = [tool_call]

    response_loop = MagicMock()
    response_loop.choices = [MagicMock(message=msg_loop)]
    response_loop.usage.prompt_tokens = 100
    response_loop.usage.completion_tokens = 10

    mock_llm_client.chat.return_value = response_loop

    # Mock MCP to always return success
    orchestrator.mcp_client.execute_tool = AsyncMock(
        return_value=MagicMock(success=True, data={}, error=None)
    )

    result = await orchestrator.process_message("user-1", "Loop test")
    assert "trouble" in result.lower() or "try again" in result.lower()
    # Should have called LLM exactly MAX_TURNS times
    assert mock_llm_client.chat.call_count == 5


@pytest.mark.asyncio
async def test_tool_error_fed_back(orchestrator, mock_mcp_client, mock_llm_client):
    """When a tool fails, the error is fed back to the LLM."""
    # First call: tool request
    tool_call = MagicMock()
    tool_call.id = "call_err"
    tool_call.function.name = "bad_tool"
    tool_call.function.arguments = json.dumps({})

    msg_tool = MagicMock()
    msg_tool.content = None
    msg_tool.tool_calls = [tool_call]

    response_1 = MagicMock()
    response_1.choices = [MagicMock(message=msg_tool)]
    response_1.usage.prompt_tokens = 100
    response_1.usage.completion_tokens = 10

    # Second call: final text after error
    msg_final = MagicMock()
    msg_final.content = "Sorry, I couldn't fetch that data."
    msg_final.tool_calls = None

    response_2 = MagicMock()
    response_2.choices = [MagicMock(message=msg_final)]
    response_2.usage.prompt_tokens = 200
    response_2.usage.completion_tokens = 20

    mock_llm_client.chat.side_effect = [response_1, response_2]

    # MCP returns error
    mock_mcp_client.execute_tool.return_value = MagicMock(
        success=False, data=None, error="Tool not found"
    )

    result = await orchestrator.process_message("user-1", "Bad request")
    assert "Sorry" in result or "couldn't" in result.lower()
```

**Step 2: Run test to verify it fails**

```bash
cd cloud-brain
python -m pytest tests/test_orchestrator.py -v
```
Expected: FAIL — Orchestrator constructor doesn't accept `llm_client` yet.

**Step 3: Upgrade the Orchestrator**

Replace contents of `cloud-brain/app/agent/orchestrator.py`:

```python
"""
Life Logger Cloud Brain — Orchestrator (AI Brain).

The Orchestrator is the central "AI Brain" that manages the LLM
conversation loop with ReAct-style function-calling. It:

1. Injects the system prompt with user context.
2. Retrieves relevant memories for context.
3. Passes available MCP tools to the LLM.
4. Executes tool calls via MCPClient (max 5 turns).
5. Feeds tool results back to the LLM.
6. Returns the final assistant response.

This replaces the Phase 1.3 scaffold with a production-ready
implementation.
"""

import json
import logging
from typing import Any

from app.agent.context_manager.memory_store import MemoryStore
from app.agent.llm_client import LLMClient
from app.agent.mcp_client import MCPClient
from app.agent.prompts.system import build_system_prompt

logger = logging.getLogger(__name__)

MAX_TOOL_TURNS = 5
"""Maximum number of LLM round-trips for tool execution.

Prevents infinite loops if the model continuously requests tools
without generating a final text response.
"""


class Orchestrator:
    """LLM Agent that orchestrates MCP tool calls with ReAct-style loop.

    Manages the full conversation lifecycle: context injection,
    LLM inference, tool execution, and response generation.

    Attributes:
        mcp_client: Routes tool calls to MCP servers.
        memory_store: Stores and retrieves long-term user context.
        llm_client: Async LLM client for chat completions.
    """

    def __init__(
        self,
        mcp_client: MCPClient,
        memory_store: MemoryStore,
        llm_client: LLMClient | None = None,
    ) -> None:
        """Create a new Orchestrator.

        Args:
            mcp_client: The tool routing client.
            memory_store: The long-term memory backend.
            llm_client: The LLM client. If None, creates a default instance.
        """
        self.mcp_client = mcp_client
        self.memory_store = memory_store
        self.llm_client = llm_client or LLMClient()

    def _build_tools_for_llm(self) -> list[dict[str, Any]]:
        """Convert MCP ToolDefinitions to OpenAI function-calling format.

        Maps the internal ToolDefinition model to the format expected
        by the OpenAI API's tools parameter.

        Returns:
            A list of tool dicts in OpenAI function-calling schema.
        """
        mcp_tools = self.mcp_client.get_all_tools()
        openai_tools = []

        for tool in mcp_tools:
            openai_tools.append({
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.input_schema,
                },
            })

        return openai_tools

    async def process_message(
        self,
        user_id: str,
        message: str,
        user_context_suffix: str | None = None,
    ) -> str:
        """Process a user message through the AI Brain.

        Implements the full ReAct-style conversation loop:
        1. Build system prompt with user context.
        2. Retrieve relevant memories.
        3. Get available tools from MCP registry.
        4. Loop: LLM inference -> tool execution -> feed results back.
        5. Return the final text response.

        Args:
            user_id: The authenticated user's ID.
            message: The user's chat message.
            user_context_suffix: Optional user profile context to append
                to the system prompt.

        Returns:
            The final assistant response text.
        """
        # 1. Build system prompt
        system_prompt = build_system_prompt(user_context_suffix)

        # 2. Retrieve relevant context from memory
        context_entries = await self.memory_store.query(
            user_id, query_text=message, limit=5
        )
        context_text = ""
        if context_entries:
            context_text = "\n\n## Relevant Context\n"
            for entry in context_entries:
                context_text += f"- {entry.get('text', '')}\n"

        # 3. Build initial messages
        messages: list[dict[str, Any]] = [
            {"role": "system", "content": system_prompt + context_text},
            {"role": "user", "content": message},
        ]

        # 4. Get available tools
        tools = self._build_tools_for_llm()

        logger.info(
            "Processing message for user '%s' with %d context items and %d tools",
            user_id,
            len(context_entries),
            len(tools),
        )

        # 5. ReAct loop (max MAX_TOOL_TURNS turns)
        for turn in range(MAX_TOOL_TURNS):
            response = await self.llm_client.chat(
                messages,
                tools=tools if tools else None,
            )
            assistant_message = response.choices[0].message

            # Check for tool calls
            if assistant_message.tool_calls:
                # Add assistant message with tool calls to history
                messages.append({
                    "role": "assistant",
                    "content": assistant_message.content,
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments,
                            },
                        }
                        for tc in assistant_message.tool_calls
                    ],
                })

                # Execute each tool call
                for tool_call in assistant_message.tool_calls:
                    func_name = tool_call.function.name
                    try:
                        arguments = json.loads(tool_call.function.arguments)
                    except json.JSONDecodeError:
                        arguments = {}

                    logger.info(
                        "Turn %d: executing tool '%s' with args %s",
                        turn + 1,
                        func_name,
                        arguments,
                    )

                    # Execute via MCP
                    result = await self.mcp_client.execute_tool(
                        func_name, arguments, user_id
                    )

                    # Build tool result message
                    if result.success:
                        result_content = json.dumps(result.data)
                    else:
                        result_content = json.dumps(
                            {"error": result.error or "Tool execution failed"}
                        )

                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": result_content,
                    })

                # Continue loop — LLM will process tool results
                continue

            # No tool calls — return final text response
            final_content = assistant_message.content or ""
            logger.info(
                "Final response for user '%s' after %d turn(s)",
                user_id,
                turn + 1,
            )
            return final_content

        # Safety: max turns exceeded
        logger.warning(
            "Max tool turns (%d) exceeded for user '%s'",
            MAX_TOOL_TURNS,
            user_id,
        )
        return (
            "I'm having trouble retrieving all the information right now. "
            "Please try again or rephrase your question."
        )
```

**Step 4: Run test to verify it passes**

```bash
cd cloud-brain
python -m pytest tests/test_orchestrator.py -v
```
Expected: 4 tests PASS

**Step 5: Update `main.py` to wire LLMClient into lifespan**

In `cloud-brain/app/main.py`:
- Add import: `from app.agent.llm_client import LLMClient`
- In `lifespan()`, after `app.state.memory_store = InMemoryStore()`, add: `app.state.llm_client = LLMClient()`

Update `cloud-brain/app/api/v1/chat.py`:
- In `_get_orchestrator()`, change to:
  ```python
  return Orchestrator(
      mcp_client=request.app.state.mcp_client,
      memory_store=request.app.state.memory_store,
      llm_client=request.app.state.llm_client,
  )
  ```
- In `websocket_chat()`, change orchestrator creation to:
  ```python
  orchestrator = Orchestrator(
      mcp_client=mcp_client,
      memory_store=memory_store,
      llm_client=app.state.llm_client,
  )
  ```

**Step 6: Run full test suite**

```bash
cd cloud-brain
python -m pytest -v
```
Expected: All existing tests still pass (test_chat.py mocks override app.state).

**Step 7: Commit**

```bash
git add -A
git commit -m "feat(1.8.3): upgrade Orchestrator with ReAct tool execution loop"
```

---

## Task 5: Cross-App Reasoning Engine (Sub-phase 1.8.4)

**Files:**
- Create: `cloud-brain/app/analytics/__init__.py`
- Create: `cloud-brain/app/analytics/reasoning_engine.py`
- Create: `cloud-brain/tests/test_reasoning_engine.py`

**Step 1: Write the failing test**

Create `cloud-brain/tests/test_reasoning_engine.py`:

```python
"""
Life Logger Cloud Brain — Reasoning Engine Tests.

Tests for cross-app analytical helpers that synthesize insights
from multiple data sources.
"""

import pytest

from app.analytics.reasoning_engine import ReasoningEngine


@pytest.fixture
def engine():
    """Create a ReasoningEngine instance."""
    return ReasoningEngine()


def test_analyze_deficit_in_deficit(engine):
    """Should detect caloric deficit correctly."""
    result = engine.analyze_deficit(
        nutrition_calories=1500, active_burn=400, bmr=1800
    )
    assert result["status"] == "deficit"
    assert result["net_calories"] < 0
    assert result["net_calories"] == 1500 - (1800 + 400)


def test_analyze_deficit_in_surplus(engine):
    """Should detect caloric surplus correctly."""
    result = engine.analyze_deficit(
        nutrition_calories=3000, active_burn=200, bmr=1800
    )
    assert result["status"] == "surplus"
    assert result["net_calories"] > 0
    assert result["net_calories"] == 3000 - (1800 + 200)


def test_analyze_deficit_extreme_deficit_warning(engine):
    """Extreme deficit should produce a warning recommendation."""
    result = engine.analyze_deficit(
        nutrition_calories=800, active_burn=500, bmr=1800
    )
    assert result["status"] == "deficit"
    assert result["magnitude"] > 500
    assert "under-eating" in result["recommendation"].lower() or "eat more" in result["recommendation"].lower()


def test_analyze_deficit_balanced(engine):
    """Near-zero net should return appropriate status."""
    result = engine.analyze_deficit(
        nutrition_calories=2200, active_burn=400, bmr=1800
    )
    assert result["net_calories"] == 0
    assert result["magnitude"] == 0


def test_correlate_sleep_and_activity_empty(engine):
    """Empty data should return a no-data message."""
    result = engine.correlate_sleep_and_activity([], [])
    assert isinstance(result, str)
    assert len(result) > 0


def test_correlate_sleep_and_activity_with_data(engine):
    """With sufficient data, should return a correlation summary."""
    sleep_data = [
        {"date": "2026-02-01", "hours": 7.5},
        {"date": "2026-02-02", "hours": 6.0},
        {"date": "2026-02-03", "hours": 8.0},
        {"date": "2026-02-04", "hours": 5.5},
        {"date": "2026-02-05", "hours": 7.0},
    ]
    activity_data = [
        {"date": "2026-02-01", "calories_burned": 400},
        {"date": "2026-02-02", "calories_burned": 600},
        {"date": "2026-02-03", "calories_burned": 300},
        {"date": "2026-02-04", "calories_burned": 700},
        {"date": "2026-02-05", "calories_burned": 450},
    ]
    result = engine.correlate_sleep_and_activity(sleep_data, activity_data)
    assert isinstance(result, str)


def test_analyze_activity_trend(engine):
    """Should detect declining activity trend."""
    this_month = [
        {"date": "2026-02-01", "type": "run"},
        {"date": "2026-02-05", "type": "run"},
        {"date": "2026-02-10", "type": "run"},
    ]
    last_month = [
        {"date": "2026-01-01", "type": "run"},
        {"date": "2026-01-05", "type": "run"},
        {"date": "2026-01-10", "type": "run"},
        {"date": "2026-01-15", "type": "run"},
        {"date": "2026-01-20", "type": "run"},
        {"date": "2026-01-25", "type": "run"},
        {"date": "2026-01-28", "type": "run"},
        {"date": "2026-01-30", "type": "run"},
    ]
    result = engine.analyze_activity_trend(this_month, last_month)
    assert result["trend"] == "declining"
    assert result["this_month_count"] == 3
    assert result["last_month_count"] == 8
```

**Step 2: Run test to verify it fails**

```bash
cd cloud-brain
python -m pytest tests/test_reasoning_engine.py -v
```
Expected: FAIL — module not found

**Step 3: Implement the reasoning engine**

Create `cloud-brain/app/analytics/__init__.py`:
```python
"""Life Logger Cloud Brain — Analytics and Reasoning Modules."""
```

Create `cloud-brain/app/analytics/reasoning_engine.py`:

```python
"""
Life Logger Cloud Brain — Cross-App Reasoning Engine.

Provides deterministic analytical helpers that synthesize data from
multiple MCP sources (Apple Health, Strava, CalAI) into higher-order
insights. These helpers can be called by the Orchestrator proactively
or registered as MCP tools.

The engine performs the *calculation* — the LLM provides the *narration*.
"""

import logging
import statistics
from typing import Any

logger = logging.getLogger(__name__)


class ReasoningEngine:
    """Analyzes cross-app data to generate higher-order insights.

    All methods are pure functions operating on provided data.
    No database or API calls — data must be pre-fetched by the caller.
    """

    def analyze_deficit(
        self,
        nutrition_calories: int,
        active_burn: int,
        bmr: int = 1800,
    ) -> dict[str, Any]:
        """Calculate caloric deficit or surplus.

        Computes net calories as intake minus total expenditure
        (BMR + active burn) and classifies the result.

        Args:
            nutrition_calories: Total calories consumed (from CalAI/Health).
            active_burn: Calories burned through exercise (from Strava/Health).
            bmr: Basal metabolic rate estimate. Defaults to 1800.

        Returns:
            A dict with keys: net_calories, status ('deficit'/'surplus'),
            magnitude (absolute value), and recommendation string.
        """
        total_out = bmr + active_burn
        net = nutrition_calories - total_out
        status = "deficit" if net < 0 else "surplus"
        magnitude = abs(net)

        if net < -500:
            recommendation = (
                "You're under-eating significantly. Eat more to sustain "
                "your activity level and avoid metabolic slowdown."
            )
        elif net < -200:
            recommendation = (
                "You're in a healthy deficit. Keep it up for steady, "
                "sustainable fat loss."
            )
        elif net <= 200:
            recommendation = (
                "You're roughly at maintenance. If your goal is weight loss, "
                "consider a moderate 300-500 cal/day deficit."
            )
        else:
            recommendation = (
                f"You're in a {magnitude} cal surplus. If weight loss is the goal, "
                "reduce portion sizes or increase activity."
            )

        return {
            "net_calories": net,
            "status": status,
            "magnitude": magnitude,
            "recommendation": recommendation,
        }

    def correlate_sleep_and_activity(
        self,
        sleep_data: list[dict[str, Any]],
        activity_data: list[dict[str, Any]],
    ) -> str:
        """Analyze correlation between sleep quality and activity levels.

        Aligns data by date and computes a simple Pearson correlation
        between sleep hours and activity calories burned.

        Args:
            sleep_data: List of dicts with 'date' and 'hours' keys.
            activity_data: List of dicts with 'date' and 'calories_burned' keys.

        Returns:
            A human-readable summary of the correlation finding.
        """
        if not sleep_data or not activity_data:
            return "Not enough data yet. Keep tracking sleep and activity for meaningful correlations."

        # Build lookup by date
        sleep_by_date = {s["date"]: s.get("hours", 0) for s in sleep_data}
        activity_by_date = {a["date"]: a.get("calories_burned", 0) for a in activity_data}

        # Find overlapping dates
        common_dates = sorted(set(sleep_by_date.keys()) & set(activity_by_date.keys()))

        if len(common_dates) < 3:
            return "Not enough overlapping data points. Keep tracking for at least a week."

        sleep_vals = [sleep_by_date[d] for d in common_dates]
        activity_vals = [activity_by_date[d] for d in common_dates]

        # Simple Pearson-like correlation
        try:
            correlation = statistics.correlation(sleep_vals, activity_vals)
        except (statistics.StatisticsError, ZeroDivisionError):
            return "Unable to compute correlation — not enough variance in the data."

        if correlation > 0.5:
            return (
                f"Moderate positive correlation ({correlation:.2f}): You tend to sleep "
                "more on days you're more active. Exercise seems to help your sleep!"
            )
        elif correlation < -0.5:
            return (
                f"Moderate negative correlation ({correlation:.2f}): You sleep less on "
                "high-activity days. Consider earlier workout times."
            )
        else:
            return (
                f"Weak correlation ({correlation:.2f}): No strong pattern between "
                "sleep and activity yet. Keep tracking!"
            )

    def analyze_activity_trend(
        self,
        this_month: list[dict[str, Any]],
        last_month: list[dict[str, Any]],
    ) -> dict[str, Any]:
        """Compare activity frequency between current and previous month.

        Args:
            this_month: List of activity dicts for the current month.
            last_month: List of activity dicts for the previous month.

        Returns:
            A dict with trend ('improving'/'declining'/'stable'),
            counts, and percentage change.
        """
        this_count = len(this_month)
        last_count = len(last_month)

        if last_count == 0:
            pct_change = 100.0 if this_count > 0 else 0.0
        else:
            pct_change = ((this_count - last_count) / last_count) * 100

        if pct_change > 10:
            trend = "improving"
        elif pct_change < -10:
            trend = "declining"
        else:
            trend = "stable"

        return {
            "trend": trend,
            "this_month_count": this_count,
            "last_month_count": last_count,
            "percent_change": round(pct_change, 1),
        }
```

**Step 4: Run test to verify it passes**

```bash
cd cloud-brain
python -m pytest tests/test_reasoning_engine.py -v
```
Expected: 7 tests PASS

**Step 5: Commit**

```bash
git add -A
git commit -m "feat(1.8.4): add cross-app reasoning engine for calorie/sleep/activity analysis"
```

---

## Task 6: Voice Input Endpoint (Sub-phase 1.8.5)

**Files:**
- Create: `cloud-brain/app/api/v1/transcribe.py`
- Create: `cloud-brain/tests/test_transcribe.py`
- Modify: `cloud-brain/app/main.py` (register router)

**Step 1: Write the failing test**

Create `cloud-brain/tests/test_transcribe.py`:

```python
"""
Life Logger Cloud Brain — Transcribe Endpoint Tests.

Tests for the voice transcription endpoint. Uses mock STT
since the endpoint is scaffolded with mock transcription.
"""

import io

import pytest
from fastapi.testclient import TestClient

from app.main import app


@pytest.fixture
def client():
    """Create a test client."""
    with TestClient(app, raise_server_exceptions=False) as c:
        yield c


def test_transcribe_valid_audio(client):
    """Valid audio upload should return transcription text."""
    audio_content = b"fake-audio-data"
    response = client.post(
        "/api/v1/transcribe",
        files={"file": ("test.wav", io.BytesIO(audio_content), "audio/wav")},
    )
    assert response.status_code == 200
    data = response.json()
    assert "text" in data
    assert isinstance(data["text"], str)
    assert len(data["text"]) > 0


def test_transcribe_invalid_format(client):
    """Invalid file format should return 400."""
    response = client.post(
        "/api/v1/transcribe",
        files={"file": ("test.txt", io.BytesIO(b"not audio"), "text/plain")},
    )
    assert response.status_code == 400


def test_transcribe_mp3_accepted(client):
    """MP3 format should be accepted."""
    response = client.post(
        "/api/v1/transcribe",
        files={"file": ("voice.mp3", io.BytesIO(b"mp3-data"), "audio/mpeg")},
    )
    assert response.status_code == 200


def test_transcribe_webm_accepted(client):
    """WebM format should be accepted."""
    response = client.post(
        "/api/v1/transcribe",
        files={"file": ("voice.webm", io.BytesIO(b"webm-data"), "audio/webm")},
    )
    assert response.status_code == 200


def test_transcribe_m4a_accepted(client):
    """M4A format should be accepted."""
    response = client.post(
        "/api/v1/transcribe",
        files={"file": ("voice.m4a", io.BytesIO(b"m4a-data"), "audio/m4a")},
    )
    assert response.status_code == 200
```

**Step 2: Run test to verify it fails**

```bash
cd cloud-brain
python -m pytest tests/test_transcribe.py -v
```
Expected: FAIL — 404 on `/api/v1/transcribe`

**Step 3: Create the transcribe endpoint**

Create `cloud-brain/app/api/v1/transcribe.py`:

```python
"""
Life Logger Cloud Brain — Voice Transcription Endpoint.

Accepts audio file uploads and returns transcribed text.
Currently uses a mock transcription; the real Whisper STT
integration will be added when infrastructure is ready.

Supported formats: .webm, .m4a, .wav, .mp3
"""

import logging
import os

from fastapi import APIRouter, HTTPException, UploadFile, status

logger = logging.getLogger(__name__)

router = APIRouter(tags=["transcribe"])

ALLOWED_EXTENSIONS = {".webm", ".m4a", ".wav", ".mp3"}
MAX_FILE_SIZE = 25 * 1024 * 1024  # 25 MB (Whisper limit)


@router.post("/transcribe")
async def transcribe_audio(file: UploadFile) -> dict[str, str]:
    """Transcribe an audio file to text.

    Accepts audio uploads in common formats, validates the file,
    and returns the transcribed text. Currently uses a mock
    transcription response.

    Args:
        file: The uploaded audio file.

    Returns:
        A dict with 'text' key containing the transcription.

    Raises:
        HTTPException: 400 if file format is invalid or file is too large.
    """
    # Validate file extension
    filename = file.filename or ""
    ext = os.path.splitext(filename)[1].lower()

    if ext not in ALLOWED_EXTENSIONS:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid file format '{ext}'. Accepted: {', '.join(sorted(ALLOWED_EXTENSIONS))}",
        )

    # Read file content
    content = await file.read()

    if len(content) > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"File too large. Maximum size: {MAX_FILE_SIZE // (1024 * 1024)} MB",
        )

    if len(content) == 0:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Empty file uploaded",
        )

    logger.info(
        "Transcription request: filename=%s, size=%d bytes, format=%s",
        filename,
        len(content),
        ext,
    )

    # --- Mock transcription ---
    # TODO(phase-1.8): Replace with real Whisper API call when ready
    text = "[Mock transcription] Audio received and would be transcribed here."

    return {"text": text}
```

**Step 4: Register the router in `main.py`**

In `cloud-brain/app/main.py`, add:
```python
from app.api.v1.transcribe import router as transcribe_router
# After integrations_router line:
app.include_router(transcribe_router, prefix="/api/v1")  # Phase 1.8.5
```

**Step 5: Run test to verify it passes**

```bash
cd cloud-brain
python -m pytest tests/test_transcribe.py -v
```
Expected: 5 tests PASS

**Step 6: Commit**

```bash
git add -A
git commit -m "feat(1.8.5): add voice transcription endpoint with mock STT scaffold"
```

---

## Task 7: User Profile & Preferences (Sub-phase 1.8.6)

**Files:**
- Modify: `cloud-brain/app/agent/context_manager/user_profile_service.py`
- Create: `cloud-brain/app/api/v1/users.py`
- Create: `cloud-brain/tests/test_user_profile.py`
- Modify: `cloud-brain/app/main.py` (register router)

Note: The `User` model at `cloud-brain/app/models/user.py:51-58` already has `coach_persona` (String, default "tough_love") and `is_premium` (Boolean, default False). No schema changes needed.

**Step 1: Write the failing test**

Create `cloud-brain/tests/test_user_profile.py`:

```python
"""
Life Logger Cloud Brain — User Profile Manager Tests.

Tests for the dynamic system prompt suffix generation.
"""

from unittest.mock import AsyncMock, MagicMock

import pytest

from app.agent.context_manager.user_profile_service import UserProfileService


@pytest.fixture
def mock_session():
    """Create a mocked async DB session."""
    return AsyncMock()


@pytest.fixture
def profile_service(mock_session):
    """Create a UserProfileService with mocked DB."""
    return UserProfileService(session=mock_session)


@pytest.mark.asyncio
async def test_get_system_prompt_suffix_tough_love(profile_service, mock_session):
    """Tough love persona should produce a tough prompt suffix."""
    mock_result = MagicMock()
    mock_row = {
        "id": "user-1",
        "email": "test@test.com",
        "coach_persona": "tough_love",
        "is_premium": False,
    }
    mock_result.mappings.return_value.first.return_value = mock_row
    mock_session.execute.return_value = mock_result

    suffix = await profile_service.get_system_prompt_suffix("user-1")
    assert isinstance(suffix, str)
    assert "tough" in suffix.lower() or "direct" in suffix.lower()


@pytest.mark.asyncio
async def test_get_system_prompt_suffix_gentle(profile_service, mock_session):
    """Gentle persona should produce a supportive prompt suffix."""
    mock_result = MagicMock()
    mock_row = {
        "id": "user-2",
        "email": "gentle@test.com",
        "coach_persona": "gentle",
        "is_premium": True,
    }
    mock_result.mappings.return_value.first.return_value = mock_row
    mock_session.execute.return_value = mock_result

    suffix = await profile_service.get_system_prompt_suffix("user-2")
    assert "gentle" in suffix.lower() or "supportive" in suffix.lower()


@pytest.mark.asyncio
async def test_get_system_prompt_suffix_not_found(profile_service, mock_session):
    """Missing user should return empty string."""
    mock_result = MagicMock()
    mock_result.mappings.return_value.first.return_value = None
    mock_session.execute.return_value = mock_result

    suffix = await profile_service.get_system_prompt_suffix("ghost-user")
    assert suffix == ""
```

**Step 2: Run test to verify it fails**

```bash
cd cloud-brain
python -m pytest tests/test_user_profile.py -v
```
Expected: FAIL — `get_system_prompt_suffix` doesn't exist yet.

**Step 3: Add `get_system_prompt_suffix` to UserProfileService**

In `cloud-brain/app/agent/context_manager/user_profile_service.py`, add this method after `get_profile`:

```python
    async def get_system_prompt_suffix(self, user_id: str) -> str:
        """Generate a system prompt suffix based on user preferences.

        Queries the user's stored persona and subscription tier to produce
        a context string that the Orchestrator appends to the base system prompt.

        Args:
            user_id: The Supabase auth UID.

        Returns:
            A prompt suffix string. Empty string if user not found.
        """
        result = await self._session.execute(
            text(
                "SELECT id, email, coach_persona, is_premium "
                "FROM users WHERE id = :uid"
            ),
            {"uid": user_id},
        )
        row = result.mappings().first()

        if row is None:
            logger.warning("No profile found for user '%s' — using defaults", user_id)
            return ""

        persona = row.get("coach_persona", "tough_love")
        is_premium = row.get("is_premium", False)

        persona_descriptions = {
            "tough_love": (
                "\n\n## User Preferences\n"
                "The user prefers a direct, tough love coaching style. "
                "Be blunt, hold them accountable, and don't sugarcoat."
            ),
            "balanced": (
                "\n\n## User Preferences\n"
                "The user prefers a balanced coaching style. "
                "Be direct but encouraging. Mix tough feedback with recognition."
            ),
            "gentle": (
                "\n\n## User Preferences\n"
                "The user prefers a gentle, supportive coaching style. "
                "Focus on progress, use encouraging language, and be patient."
            ),
        }

        suffix = persona_descriptions.get(persona, persona_descriptions["tough_love"])

        tier = "Premium" if is_premium else "Free"
        suffix += f"\nUser tier: {tier}."

        return suffix
```

**Step 4: Create Users API endpoint**

Create `cloud-brain/app/api/v1/users.py`:

```python
"""
Life Logger Cloud Brain — User Preferences API.

Endpoints for reading and updating user profile preferences
such as coaching persona and subscription tier.
"""

import logging

from fastapi import APIRouter, Depends, HTTPException, Request, status
from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer
from pydantic import BaseModel
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

from app.database import get_db
from app.services.auth_service import AuthService

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/users", tags=["users"])
security = HTTPBearer()

VALID_PERSONAS = {"tough_love", "balanced", "gentle"}


class UpdatePreferencesRequest(BaseModel):
    """Request body for updating user preferences.

    Attributes:
        coach_persona: The coaching style preference.
    """

    coach_persona: str | None = None


def _get_auth_service(request: Request) -> AuthService:
    """Retrieve the shared AuthService from app state."""
    return request.app.state.auth_service


@router.get("/me/preferences")
async def get_preferences(
    credentials: HTTPAuthorizationCredentials = Depends(security),
    auth_service: AuthService = Depends(_get_auth_service),
    db: AsyncSession = Depends(get_db),
) -> dict:
    """Get the current user's AI preferences."""
    user = await auth_service.get_user(credentials.credentials)
    user_id = user.get("id", "unknown")

    result = await db.execute(
        text("SELECT coach_persona, is_premium FROM users WHERE id = :uid"),
        {"uid": user_id},
    )
    row = result.mappings().first()

    if row is None:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="User not found")

    return {"coach_persona": row["coach_persona"], "is_premium": row["is_premium"]}


@router.patch("/me/preferences")
async def update_preferences(
    body: UpdatePreferencesRequest,
    credentials: HTTPAuthorizationCredentials = Depends(security),
    auth_service: AuthService = Depends(_get_auth_service),
    db: AsyncSession = Depends(get_db),
) -> dict:
    """Update the current user's AI preferences."""
    user = await auth_service.get_user(credentials.credentials)
    user_id = user.get("id", "unknown")

    if body.coach_persona and body.coach_persona not in VALID_PERSONAS:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid persona. Must be one of: {', '.join(sorted(VALID_PERSONAS))}",
        )

    if not body.coach_persona:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="No fields to update")

    await db.execute(
        text("UPDATE users SET coach_persona = :persona WHERE id = :uid"),
        {"persona": body.coach_persona, "uid": user_id},
    )
    await db.commit()

    return {"message": "Preferences updated", "coach_persona": body.coach_persona}
```

Register in `main.py`:
```python
from app.api.v1.users import router as users_router
app.include_router(users_router, prefix="/api/v1")  # Phase 1.8.6
```

**Step 5: Run test to verify it passes**

```bash
cd cloud-brain
python -m pytest tests/test_user_profile.py -v
```
Expected: 3 tests PASS

**Step 6: Commit**

```bash
git add -A
git commit -m "feat(1.8.6): add user profile preferences with dynamic system prompt suffix"
```

---

## Task 8: Rate Limiter Service (Sub-phase 1.8.9)

**Files:**
- Create: `cloud-brain/app/services/rate_limiter.py`
- Create: `cloud-brain/tests/test_rate_limiter.py`

**Step 1: Write the failing test**

Create `cloud-brain/tests/test_rate_limiter.py`:

```python
"""
Life Logger Cloud Brain — Rate Limiter Service Tests.

Tests the Redis-backed per-user rate limiting with tiered limits.
Uses a mocked Redis client.
"""

from unittest.mock import AsyncMock, patch

import pytest

from app.services.rate_limiter import RateLimiter


@pytest.fixture
def mock_redis():
    """Create a mocked async Redis client."""
    r = AsyncMock()
    r.incr = AsyncMock(return_value=1)
    r.expire = AsyncMock()
    return r


@pytest.fixture
def limiter(mock_redis):
    """Create a RateLimiter with mocked Redis."""
    with patch("app.services.rate_limiter.settings") as mock_settings:
        mock_settings.redis_url = "redis://localhost:6379/0"
        rl = RateLimiter()
        rl._redis = mock_redis
        yield rl


@pytest.mark.asyncio
async def test_first_request_allowed(limiter, mock_redis):
    """First request of the day should be allowed."""
    mock_redis.incr.return_value = 1
    result = await limiter.check_limit("user-1", tier="free")
    assert result.allowed is True
    assert result.remaining == 49


@pytest.mark.asyncio
async def test_free_tier_limit_exceeded(limiter, mock_redis):
    """Free tier should be blocked after 50 requests."""
    mock_redis.incr.return_value = 51
    result = await limiter.check_limit("user-1", tier="free")
    assert result.allowed is False
    assert result.remaining == 0


@pytest.mark.asyncio
async def test_premium_tier_higher_limit(limiter, mock_redis):
    """Premium tier should allow up to 500 requests."""
    mock_redis.incr.return_value = 100
    result = await limiter.check_limit("user-1", tier="premium")
    assert result.allowed is True
    assert result.remaining == 400


@pytest.mark.asyncio
async def test_premium_tier_limit_exceeded(limiter, mock_redis):
    """Premium tier should be blocked after 500 requests."""
    mock_redis.incr.return_value = 501
    result = await limiter.check_limit("user-1", tier="premium")
    assert result.allowed is False


@pytest.mark.asyncio
async def test_redis_key_expires(limiter, mock_redis):
    """First request should set TTL on the Redis key."""
    mock_redis.incr.return_value = 1
    await limiter.check_limit("user-1", tier="free")
    mock_redis.expire.assert_called_once()
```

**Step 2: Run test to verify it fails**

```bash
cd cloud-brain
python -m pytest tests/test_rate_limiter.py -v
```
Expected: FAIL — module not found

**Step 3: Implement rate limiter service**

Create `cloud-brain/app/services/rate_limiter.py`:

```python
"""
Life Logger Cloud Brain — Per-User Rate Limiter Service.

Redis-backed fixed-window counter for enforcing subscription-tier
rate limits on LLM endpoints. Works alongside the existing slowapi
IP-level rate limiter for different concerns:

- slowapi: IP-level abuse prevention (brute force, DDoS)
- RateLimiter: Per-user LLM cost control (Free vs Premium tiers)
"""

import logging
import time
from dataclasses import dataclass

import redis.asyncio as redis

from app.config import settings

logger = logging.getLogger(__name__)

TIER_LIMITS: dict[str, int] = {
    "free": 50,
    "premium": 500,
}


@dataclass
class RateLimitResult:
    """Result of a rate limit check.

    Attributes:
        allowed: Whether the request is within limits.
        limit: The maximum requests allowed for the tier.
        remaining: How many requests remain in the current window.
        reset_seconds: Seconds until the window resets.
    """

    allowed: bool
    limit: int
    remaining: int
    reset_seconds: int


class RateLimiter:
    """Redis-backed fixed-window rate limiter.

    Uses daily keys (keyed by user_id + day) with atomic INCR
    to count requests. Each key auto-expires after 24 hours.
    """

    def __init__(self) -> None:
        """Initialize the rate limiter with a Redis connection."""
        self._redis: redis.Redis = redis.from_url(
            settings.redis_url, decode_responses=True
        )

    async def check_limit(self, user_id: str, tier: str = "free") -> RateLimitResult:
        """Check and increment the rate limit counter for a user.

        Args:
            user_id: The authenticated user's ID.
            tier: Subscription tier ('free' or 'premium').

        Returns:
            A RateLimitResult with the check outcome.
        """
        limit = TIER_LIMITS.get(tier, TIER_LIMITS["free"])
        day_key = int(time.time() // 86400)
        redis_key = f"rate_limit:{user_id}:{day_key}"
        reset_seconds = 86400 - int(time.time() % 86400)

        try:
            current = await self._redis.incr(redis_key)
            if current == 1:
                await self._redis.expire(redis_key, 86400)

            allowed = current <= limit
            remaining = max(0, limit - current)

            if not allowed:
                logger.warning(
                    "Rate limit exceeded: user=%s tier=%s count=%d/%d",
                    user_id, tier, current, limit,
                )

            return RateLimitResult(
                allowed=allowed, limit=limit,
                remaining=remaining, reset_seconds=reset_seconds,
            )
        except redis.RedisError as exc:
            logger.error("Redis error in rate limiter: %s", exc)
            return RateLimitResult(
                allowed=False, limit=limit,
                remaining=0, reset_seconds=reset_seconds,
            )

    async def close(self) -> None:
        """Close the Redis connection."""
        await self._redis.close()
```

**Step 4: Run test to verify it passes**

```bash
cd cloud-brain
python -m pytest tests/test_rate_limiter.py -v
```
Expected: 5 tests PASS

**Step 5: Commit**

```bash
git add -A
git commit -m "feat(1.8.9): add Redis-backed per-user rate limiter with tiered limits"
```

---

## Task 9: Usage Tracker Service (Sub-phase 1.8.10)

**Files:**
- Create: `cloud-brain/app/models/usage_log.py`
- Create: `cloud-brain/app/services/usage_tracker.py`
- Create: `cloud-brain/tests/test_usage_tracker.py`

**Step 1: Write the failing test**

Create `cloud-brain/tests/test_usage_tracker.py`:

```python
"""
Life Logger Cloud Brain — Usage Tracker Service Tests.
"""

from unittest.mock import AsyncMock, MagicMock

import pytest

from app.services.usage_tracker import UsageTracker


@pytest.fixture
def mock_session():
    """Create a mocked async DB session."""
    session = AsyncMock()
    session.add = MagicMock()
    session.commit = AsyncMock()
    return session


@pytest.fixture
def tracker(mock_session):
    """Create a UsageTracker with mocked DB."""
    return UsageTracker(session=mock_session)


@pytest.mark.asyncio
async def test_track_usage(tracker, mock_session):
    """track() should add a usage record to the session."""
    await tracker.track(
        user_id="user-1", model="moonshot/kimi-k2.5",
        input_tokens=100, output_tokens=50,
    )
    mock_session.add.assert_called_once()
    mock_session.commit.assert_called_once()


@pytest.mark.asyncio
async def test_track_extracts_from_response(tracker):
    """track_from_response() should parse the OpenAI response usage."""
    mock_response = MagicMock()
    mock_response.usage.prompt_tokens = 200
    mock_response.usage.completion_tokens = 80
    mock_response.model = "moonshot/kimi-k2.5"

    await tracker.track_from_response("user-1", mock_response)
    assert tracker._session.add.called


@pytest.mark.asyncio
async def test_track_zero_tokens(tracker, mock_session):
    """Zero token usage should still be recorded."""
    await tracker.track(
        user_id="user-1", model="moonshot/kimi-k2.5",
        input_tokens=0, output_tokens=0,
    )
    mock_session.add.assert_called_once()
```

**Step 2: Run test to verify it fails**

```bash
cd cloud-brain
python -m pytest tests/test_usage_tracker.py -v
```
Expected: FAIL

**Step 3: Create UsageLog model and UsageTracker service**

Create `cloud-brain/app/models/usage_log.py`:

```python
"""
Life Logger Cloud Brain — Usage Log Model.

Tracks per-request LLM token consumption for cost analysis.
"""

import uuid

from sqlalchemy import DateTime, Integer, String
from sqlalchemy.orm import Mapped, mapped_column
from sqlalchemy.sql import func

from app.database import Base


class UsageLog(Base):
    """A single LLM usage record.

    Attributes:
        id: Unique record identifier.
        user_id: The user who made the request.
        model: The LLM model used.
        input_tokens: Prompt tokens consumed.
        output_tokens: Completion tokens generated.
        created_at: Timestamp of the API call.
    """

    __tablename__ = "usage_logs"

    id: Mapped[str] = mapped_column(
        String, primary_key=True, default=lambda: str(uuid.uuid4())
    )
    user_id: Mapped[str] = mapped_column(String, index=True)
    model: Mapped[str] = mapped_column(String)
    input_tokens: Mapped[int] = mapped_column(Integer, default=0)
    output_tokens: Mapped[int] = mapped_column(Integer, default=0)
    created_at: Mapped[str] = mapped_column(
        DateTime(timezone=True), server_default=func.now()
    )
```

Create `cloud-brain/app/services/usage_tracker.py`:

```python
"""
Life Logger Cloud Brain — Usage Tracker Service.

Tracks per-request LLM token consumption by parsing the 'usage'
field from OpenAI-compatible API responses.
"""

import logging
from typing import Any

from sqlalchemy.ext.asyncio import AsyncSession

from app.models.usage_log import UsageLog

logger = logging.getLogger(__name__)


class UsageTracker:
    """Tracks LLM token usage per request.

    Attributes:
        _session: The async database session.
    """

    def __init__(self, session: AsyncSession) -> None:
        """Create a new UsageTracker.

        Args:
            session: An async SQLAlchemy session.
        """
        self._session = session

    async def track(
        self, user_id: str, model: str,
        input_tokens: int, output_tokens: int,
    ) -> None:
        """Record a single LLM usage event.

        Args:
            user_id: The user who triggered the request.
            model: The LLM model identifier.
            input_tokens: Prompt tokens consumed.
            output_tokens: Completion tokens generated.
        """
        log = UsageLog(
            user_id=user_id, model=model,
            input_tokens=input_tokens, output_tokens=output_tokens,
        )
        self._session.add(log)
        await self._session.commit()
        logger.info(
            "Usage tracked: user=%s model=%s in=%d out=%d",
            user_id, model, input_tokens, output_tokens,
        )

    async def track_from_response(self, user_id: str, response: Any) -> None:
        """Extract usage from an OpenAI response and record it.

        Args:
            user_id: The user who triggered the request.
            response: The ChatCompletion response from the OpenAI SDK.
        """
        usage = getattr(response, "usage", None)
        model = getattr(response, "model", "unknown")
        input_tokens = getattr(usage, "prompt_tokens", 0) if usage else 0
        output_tokens = getattr(usage, "completion_tokens", 0) if usage else 0
        await self.track(user_id, model, input_tokens, output_tokens)
```

**Step 4: Run test to verify it passes**

```bash
cd cloud-brain
python -m pytest tests/test_usage_tracker.py -v
```
Expected: 3 tests PASS

**Step 5: Commit**

```bash
git add -A
git commit -m "feat(1.8.10): add UsageLog model and UsageTracker service"
```

---

## Task 10: Rate Limiter Middleware (Sub-phase 1.8.11)

**Files:**
- Create: `cloud-brain/app/api/deps.py`
- Modify: `cloud-brain/app/main.py`
- Create: `cloud-brain/tests/test_rate_limit_middleware.py`

**Step 1: Write the failing test**

Create `cloud-brain/tests/test_rate_limit_middleware.py`:

```python
"""
Life Logger Cloud Brain — Rate Limit Middleware Tests.
"""

from unittest.mock import AsyncMock, MagicMock

import pytest
from fastapi import HTTPException

from app.api.deps import check_rate_limit
from app.services.rate_limiter import RateLimitResult


@pytest.mark.asyncio
async def test_rate_limit_allows_request():
    """Request within limits should pass through."""
    mock_limiter = AsyncMock()
    mock_limiter.check_limit.return_value = RateLimitResult(
        allowed=True, limit=50, remaining=49, reset_seconds=3600
    )
    mock_user = {"id": "user-1"}
    mock_db = AsyncMock()
    mock_result = MagicMock()
    mock_result.scalar_one_or_none.return_value = False
    mock_db.execute.return_value = mock_result

    result = await check_rate_limit(user=mock_user, limiter=mock_limiter, db=mock_db)
    assert result is None


@pytest.mark.asyncio
async def test_rate_limit_blocks_exceeded():
    """Request exceeding limit should raise 429."""
    mock_limiter = AsyncMock()
    mock_limiter.check_limit.return_value = RateLimitResult(
        allowed=False, limit=50, remaining=0, reset_seconds=3600
    )
    mock_user = {"id": "user-1"}
    mock_db = AsyncMock()
    mock_result = MagicMock()
    mock_result.scalar_one_or_none.return_value = False
    mock_db.execute.return_value = mock_result

    with pytest.raises(HTTPException) as exc_info:
        await check_rate_limit(user=mock_user, limiter=mock_limiter, db=mock_db)
    assert exc_info.value.status_code == 429
```

**Step 2: Run test to verify it fails**

```bash
cd cloud-brain
python -m pytest tests/test_rate_limit_middleware.py -v
```
Expected: FAIL

**Step 3: Create the deps module**

Create `cloud-brain/app/api/deps.py`:

```python
"""
Life Logger Cloud Brain — Shared API Dependencies.

FastAPI dependencies for rate limiting and authentication.
"""

import logging
from typing import Any

from fastapi import HTTPException, status
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.user import User
from app.services.rate_limiter import RateLimiter

logger = logging.getLogger(__name__)


async def check_rate_limit(
    user: dict[str, Any],
    limiter: RateLimiter,
    db: AsyncSession,
) -> None:
    """Enforce per-user rate limits based on subscription tier.

    Args:
        user: The authenticated user dict.
        limiter: The rate limiter service.
        db: The async database session.

    Raises:
        HTTPException: 429 if the daily limit is exceeded.
    """
    user_id = user.get("id", "unknown")

    result = await db.execute(select(User.is_premium).where(User.id == user_id))
    is_premium = result.scalar_one_or_none() or False
    tier = "premium" if is_premium else "free"

    limit_result = await limiter.check_limit(user_id, tier=tier)

    if not limit_result.allowed:
        logger.warning("Rate limit hit: user=%s tier=%s", user_id, tier)
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail=(
                f"Daily rate limit exceeded. Your {tier} plan allows "
                f"{limit_result.limit} requests/day. Upgrade to Premium for more."
            ),
            headers={
                "X-RateLimit-Limit": str(limit_result.limit),
                "X-RateLimit-Remaining": "0",
                "X-RateLimit-Reset": str(limit_result.reset_seconds),
                "Retry-After": str(limit_result.reset_seconds),
            },
        )
```

**Step 4: Wire into `main.py` lifespan**

In `cloud-brain/app/main.py`:
```python
from app.services.rate_limiter import RateLimiter

# In lifespan() startup, after llm_client:
app.state.rate_limiter = RateLimiter()

# In lifespan() shutdown, before http_client.aclose():
await app.state.rate_limiter.close()
```

**Step 5: Run test to verify it passes**

```bash
cd cloud-brain
python -m pytest tests/test_rate_limit_middleware.py -v
```
Expected: 2 tests PASS

**Step 6: Run full test suite**

```bash
cd cloud-brain
python -m pytest -v
```
Expected: All tests pass.

**Step 7: Commit**

```bash
git add -A
git commit -m "feat(1.8.11): add rate limit middleware with tier-based enforcement"
```

---

## Task 11: Test Harness AI Chat UI (Sub-phase 1.8.7)

**Files:**
- Modify: `life_logger/lib/features/harness/harness_screen.dart`

**Step 1: Add AI Brain section to harness**

The current harness already has a CHAT section. Add a new "AI BRAIN" section below it with:

1. An `_buildAiBrainSection()` method with:
   - "Test AI Chat" button: Sends "How are my steps today?" through the WebSocket
   - "Voice Test" button: Makes HTTP POST to `/api/v1/transcribe` with mock audio
   - "Debug JSON" toggle: Shows raw JSON tool calls in the output log

2. Add corresponding action methods:
   - `_testAiChat()`: Sends a test message and logs the full response
   - `_testVoiceUpload()`: HTTP POST with mock audio bytes to transcribe endpoint

3. Wire `_buildAiBrainSection()` into the `ListView` children in `build()`, between chat and the bottom.

**Step 2: Run `flutter analyze`**

```bash
cd life_logger
flutter analyze
```
Expected: 0 issues

**Step 3: Commit**

```bash
git add -A
git commit -m "feat(1.8.7): add AI Brain test section to developer harness"
```

---

## Task 12: Update Integration Document (Sub-phase 1.8.8)

**Files:**
- Modify: `docs/plans/backend/integrations/ai-brain-integration.md`

The document already exists (368 lines). Update with:
- Actual rate limit tiers (Free: 50/day, Premium: 500/day)
- ReAct loop implementation (5 turns max)
- Usage tracking (PostgreSQL `usage_logs` table)
- Note about `openai` SDK usage instead of raw httpx

**Step 1: Update the document**

**Step 2: Commit**

```bash
git add -A
git commit -m "docs(1.8.8): update AI Brain integration doc with implementation details"
```

---

## Task 13: Full Integration Test & Lint

**Step 1: Run full Python test suite**

```bash
cd cloud-brain
python -m pytest -v --tb=short
```
Expected: All tests pass (~123 total)

**Step 2: Run ruff linter**

```bash
cd cloud-brain
ruff check . --fix && ruff format .
```
Expected: 0 errors

**Step 3: Run Flutter analyze**

```bash
cd life_logger
flutter analyze
```
Expected: 0 issues

**Step 4: Commit any lint fixes**

```bash
git add -A
git commit -m "style: fix lint issues from Phase 1.8 implementation"
```

---

## Task 14: Create Executed Phase Documentation

**Files:**
- Create: `docs/agent-executed/backend/phases/executed-phase-1.8-ai-brain.md`

Follow AGENTS.md Rule 18:
- Summary: What was built across all 11 sub-phases
- Deviations: openai SDK vs httpx, existing integration doc, mock STT
- Architecture: ReAct loop, dual rate limiters, tiered access
- Test count: Final passing count
- Next steps: Phase 1.9 orchestrator integration, Phase 1.11 analytics expansion

**Step 1: Write the document**

**Step 2: Commit**

```bash
git add -A
git commit -m "docs: add executed phase 1.8 documentation"
```

---

## Task 15: Final Verification

**Step 1: Run everything**

```bash
cd cloud-brain && python -m pytest -v && ruff check .
cd ../life_logger && flutter analyze
```

**Step 2: Review git log**

```bash
git log --oneline feat/phase-1.8-ai-brain
```

Verify atomic commits with descriptive messages.

---

## Dependency Graph

```
Task 1 (Branch + Deps)
  |-- Task 2 (LLM Client)
  |     \-- Task 4 (Orchestrator) <-- depends on 2 + 3
  |-- Task 3 (System Prompt)
  |     \-- Task 4 (Orchestrator)
  |-- Task 5 (Reasoning Engine) <-- independent
  |-- Task 6 (Voice Input) <-- independent
  |-- Task 7 (User Profile) <-- depends on 3
  |-- Task 8 (Rate Limiter) <-- independent
  |     \-- Task 10 (Middleware) <-- depends on 8
  |-- Task 9 (Usage Tracker) <-- independent
  \-- Task 11 (Harness UI) <-- depends on 4
Task 12 (Integration Doc) <-- depends on all backend tasks
Task 13 (Integration Test) <-- depends on all
Task 14 (Executed Doc) <-- depends on 13
Task 15 (Final Verification) <-- depends on 14
```

## Parallelizable Groups

After Task 1 (branch setup), these groups can run in parallel:
- **Group A (Core AI):** Task 2 -> Task 3 -> Task 4 (sequential)
- **Group B (Analytics):** Task 5 (independent)
- **Group C (Voice):** Task 6 (independent)
- **Group D (User Profile):** Task 7 (after Task 3)
- **Group E (Rate Limiting):** Task 8 + Task 9 (parallel), then Task 10 (sequential after 8)

Task 11 depends on Task 4 completing. Tasks 12-15 are sequential after all implementation.
